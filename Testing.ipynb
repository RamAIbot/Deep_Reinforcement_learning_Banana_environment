{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self,state_size,action_size,seed,fc1_units=64,fc2_units=64):\n",
    "        super(QNetwork,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size,fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units,fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,action_size)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3832, 0.5955, 0.8233],\n",
      "        [0.9287, 1.2914, 1.8397]], grad_fn=<AddmmBackward>)\n",
      "torch.return_types.max(\n",
      "values=tensor([0.8233, 1.8397]),\n",
      "indices=tensor([2, 2]))\n",
      "tensor([0.8233, 1.8397])\n",
      "tensor([[0.8233, 1.8397]])\n",
      "tensor([[0.8233],\n",
      "        [1.8397]])\n",
      "-------------\n",
      "tensor([[0.3832, 0.5955, 0.8233],\n",
      "        [0.9287, 1.2914, 1.8397]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.3832, 0.3832, 0.3832],\n",
      "        [1.2914, 1.2914, 1.2914]], grad_fn=<GatherBackward>)\n",
      "tensor([[0.3832],\n",
      "        [1.2914]], grad_fn=<GatherBackward>)\n"
     ]
    }
   ],
   "source": [
    "q = QNetwork(4,3,10)\n",
    "u = q.forward(torch.from_numpy(np.array([[1,2,3,4],[5,6,7,8]],dtype='float32')))\n",
    "print(u)\n",
    "print(u.detach().max(1))\n",
    "print(u.detach().max(1)[0])\n",
    "print(u.detach().max(1)[0].unsqueeze(0))\n",
    "print(u.detach().max(1)[0].unsqueeze(1))\n",
    "\n",
    "print('-------------')\n",
    "v = q.forward(torch.from_numpy(np.array([[1,2,3,4],[5,6,7,8]],dtype='float32')))\n",
    "print(v)\n",
    "print(v.gather(1,torch.tensor([[0,0,0],[1,1,1]])))\n",
    "print(v.gather(1,torch.tensor([[0],[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR = 5e-4\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self,action_size,buffer_size,batch_size,seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\",field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        e = self.experience(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([1 if e.done else 0 for e in experiences if e is not None])).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self,state_size,action_size,seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.qnetwork_local = QNetwork(state_size,action_size,seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size,action_size,seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)\n",
    "        \n",
    "        self.memory = ReplayBuffer(action_size,BUFFER_SIZE,BATCH_SIZE,seed)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self,state,action,reward,next_state,done):\n",
    "        self.memory.add(state,action,reward,next_state,done)\n",
    "        \n",
    "        #learn after UPDATE_EVERY time\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        self.t_step == 0\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences,GAMMA)\n",
    "                \n",
    "    def learn(self,experiences,gamma):\n",
    "        states,actions,rewards,next_states,dones = experiences\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.qnetwork_local(states).gather(1,actions)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected,Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.qnetwork_local,self.qnetwork_target,TAU)\n",
    "        \n",
    "    def soft_update(self,local_model,target_model,tau):\n",
    "        \n",
    "        for target_param,local_param in zip(target_model.parameters(),local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0 - tau)*target_param.data)\n",
    "            \n",
    "    def act(self,state,eps=0.):\n",
    "        #epsilon greedy policy\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=\"./Banana_Windows_x86_64\\Banana_Windows_x86_64/Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent = Agent(state_size,action_size,seed=0)\n",
    "trained_model = QNetwork(state_size, action_size,0).to(device)\n",
    "trained_model.load_state_dict(torch.load('./modelloss14.pth'))\n",
    "trained_model.eval()\n",
    "test_agent.qnetwork_local = trained_model\n",
    "test_agent.qnetwork_local = test_agent.qnetwork_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.0\n",
      "Score: 17.0\n",
      "Score: 15.0\n",
      "Score: 12.0\n",
      "Score: 17.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    while True:\n",
    "        action = torch.argmax(test_agent.qnetwork_local.forward(torch.from_numpy(np.array(state,dtype='float32')).to(device))) # select an action\n",
    "        env_info = env.step(action.item())[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "    print(\"Score: {}\".format(score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
